{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import json\n",
        "import random\n",
        "import shutil\n",
        "\n",
        "# ref: https://github.com/MuhammedBuyukkinaci/TensorFlow-Binary-Image-Classification-using-CNN-s/blob/master/Binary_classification.ipynb\n",
        "# tensorflow version 1.5\n",
        "\n",
        "# dataset can be downloaded from http://mrl.cs.vsb.cz/eyedataset\n",
        "def preprocess(directory_path=\"data/mrlEyes_2018_01\"):\n",
        "    dir_list = os.listdir(directory_path)\n",
        "    image_dictionary = {}\n",
        "    for dir in dir_list:\n",
        "        dir = os.path.join(directory_path, dir)\n",
        "        image_list = os.listdir(dir)\n",
        "        for image in image_list:\n",
        "            info = image.split(\"_\")\n",
        "            if int(info[3]) == 1: continue  # if wearing glasses\n",
        "            if int(info[5]) == 2: continue  # if high reflection\n",
        "            if int(info[6]) == 0: continue  # if lighting condition in bad\n",
        "            if random.random() > 0.4: continue  # 20% sampling\n",
        "            image_dictionary[os.path.join(dir, image)] = int(info[4])\n",
        "\n",
        "    print(image_dictionary)\n",
        "    resized_image_dictionary = {}\n",
        "    shutil.rmtree('data/pupil_data')\n",
        "    os.mkdir('data/pupil_data')\n",
        "\n",
        "    image_label_list = []\n",
        "\n",
        "    for image in image_dictionary.keys():\n",
        "        open_closed = image_dictionary[image]\n",
        "        img = cv2.imread(image, 0)\n",
        "        if img is None: continue\n",
        "        if img.shape[0] < 50 or img.shape[1] < 50: continue  # discard too small images\n",
        "        img = cv2.resize(img, (50, 50), interpolation=cv2.INTER_AREA)\n",
        "        _, dir, dir2, _ = image.split(\"/\")\n",
        "        image = image.replace(dir + '/' + dir2, \"pupil_data\")\n",
        "        cv2.imwrite(image, img)\n",
        "        resized_image_dictionary[image] = open_closed\n",
        "        image_label_list.append([img, open_closed])\n",
        "\n",
        "    with open('data/pupil_data_list.json', 'w') as f:\n",
        "        json.dump(resized_image_dictionary, f)\n",
        "\n",
        "    random.shuffle(image_label_list)\n",
        "    train_image_list = image_label_list[:6000]\n",
        "    test_image_list = image_label_list[6000:8000]\n",
        "\n",
        "    train_images = np.array([x[0] for x in train_image_list])\n",
        "    train_images = train_images / 255\n",
        "    train_images = train_images.reshape((-1, 50, 50, 1))\n",
        "    train_labels = np.array([[float(x[1])] for x in train_image_list])\n",
        "\n",
        "    with open('data/train_image.npy', 'wb') as f:\n",
        "        np.save(f, train_images)\n",
        "    with open('data/train_label.npy', 'wb') as f:\n",
        "        np.save(f, train_labels)\n",
        "\n",
        "    test_images = np.array([x[0] for x in test_image_list])\n",
        "    test_images = test_images / 255\n",
        "    test_images = test_images.reshape((-1, 50, 50, 1))\n",
        "    test_labels = np.array([[float(x[1])] for x in test_image_list])\n",
        "\n",
        "    with open('data/test_image.npy', 'wb') as f:\n",
        "        np.save(f, test_images)\n",
        "    with open('data/test_label.npy', 'wb') as f:\n",
        "        np.save(f, test_labels)\n",
        "\n",
        "\n",
        "preprocess()\n",
        "\n",
        "\n",
        "def simple_model(X, training=False):\n",
        "    nodes_fc1 = 512\n",
        "    # CONVOLUTION LAYER 1\n",
        "    # Weights for layer 1\n",
        "    w_1 = tf.Variable(tf.truncated_normal([11, 11, 1, 48], stddev=0.01))\n",
        "    # Bias for layer 1\n",
        "    b_1 = tf.Variable(tf.constant(0.0, shape=[[11, 11, 1, 48][3]]))\n",
        "    # Applying convolution\n",
        "    c_1 = tf.nn.conv2d(X, w_1, strides=[1, 1, 1, 1], padding='VALID')\n",
        "    # Adding bias\n",
        "    c_1 = c_1 + b_1\n",
        "    # Applying RELU\n",
        "    c_1 = tf.nn.relu(c_1)\n",
        "\n",
        "    # POOLING LAYER1\n",
        "    p_1 = tf.nn.max_pool(c_1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
        "\n",
        "    # CONVOLUTION LAYER 2\n",
        "    # Weights for layer 2\n",
        "    w_2 = tf.Variable(tf.truncated_normal([5, 5, 48, 96], stddev=0.01))\n",
        "    # Bias for layer 2\n",
        "    b_2 = tf.Variable(tf.constant(1.0, shape=[[5, 5, 48, 96][3]]))\n",
        "    # Applying convolution\n",
        "    c_2 = tf.nn.conv2d(p_1, w_2, strides=[1, 1, 1, 1], padding='SAME')\n",
        "    # Adding bias\n",
        "    c_2 = c_2 + b_2\n",
        "    # Applying RELU\n",
        "    c_2 = tf.nn.relu(c_2)\n",
        "\n",
        "    # POOLING LAYER2\n",
        "    p_2 = tf.nn.max_pool(c_2, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
        "\n",
        "    # Flattening\n",
        "    flattened = tf.reshape(p_2,[-1,9*9*96])\n",
        "\n",
        "    # Fully Connected Layer 1\n",
        "    # Getting input nodes in FC layer 1\n",
        "    input_size = int(flattened.get_shape()[1])\n",
        "    # Weights for FC Layer 1\n",
        "    w1_fc = tf.Variable(tf.truncated_normal([input_size, nodes_fc1], stddev=0.01))\n",
        "    # Bias for FC Layer 1\n",
        "    b1_fc = tf.Variable(tf.constant(1.0, shape=[nodes_fc1]))\n",
        "    # Summing Matrix calculations and bias\n",
        "    s_fc1 = tf.matmul(flattened, w1_fc) + b1_fc\n",
        "    # Applying RELU\n",
        "    s_fc1 = tf.nn.relu(s_fc1)\n",
        "\n",
        "    # Dropout Layer is not supported for vitis-ai\n",
        "\n",
        "    # Fully Connected Layer 3\n",
        "    # Weights for FC Layer 3\n",
        "    w3_fc = tf.Variable(tf.truncated_normal([nodes_fc1, 1], stddev=0.01))\n",
        "    # Bias for FC Layer 3b3_fc = tf.Variable( tf.constant(1.0, shape=[output_classes] ) )\n",
        "    b3_fc = tf.Variable(tf.constant(1.0, shape=[1]))\n",
        "    y_pred = tf.matmul(s_fc1, w3_fc) + b3_fc\n",
        "\n",
        "    if training: return y_pred\n",
        "    y_pred = tf.nn.relu(y_pred, name=\"final_output\")\n",
        "    return y_pred\n",
        "\n",
        "\n",
        "# train the model\n",
        "train_images = np.load('data/train_image.npy')\n",
        "train_labels = np.load('data/train_label.npy')\n",
        "train_size = len(train_images)\n",
        "batch_x, batch_y = None, None\n",
        "learning_rate = 1e-3\n",
        "batch_size = 8\n",
        "epoch = 30\n",
        "save_point = 30\n",
        "checkpoint_dir_path = 'data/'\n",
        "with tf.device('/device:XLA_GPU:0'):\n",
        "    with tf.Graph().as_default():\n",
        "        X = tf.placeholder(tf.float32, shape=[None, 50, 50, 1], name=\"normalized_gray_image\")\n",
        "        y_true = tf.placeholder(tf.float32, shape=[None, 1], name=\"output\")\n",
        "        y_pred = simple_model(X, True)\n",
        "        # loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_true, logits=y_pred))\n",
        "        loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=y_true, logits=y_pred))\n",
        "        global_step = tf.contrib.framework.get_or_create_global_step()\n",
        "        train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
        "        init = tf.global_variables_initializer()\n",
        "        saver = tf.train.Saver()\n",
        "        with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
        "            sess.run(init)\n",
        "            cur_loss = 0\n",
        "            for i in range(1, epoch + 1):\n",
        "                batch_epoch_size = int(len(train_images) / batch_size)\n",
        "                for b_num in range(batch_epoch_size):\n",
        "                    offset = b_num * batch_size\n",
        "                    if offset + batch_size < train_size:\n",
        "                        batch_x, batch_y = train_images[offset:(offset + batch_size)], train_labels[\n",
        "                                                                             offset:(offset + batch_size)]\n",
        "                    else:\n",
        "                        batch_x, batch_y = train_images[offset:], train_labels[offset:]\n",
        "                    _, cur_loss = sess.run([train_op, loss],\n",
        "                                                    feed_dict={X: batch_x, y_true: batch_y})\n",
        "                print(\"testing loss : \", i, cur_loss)\n",
        "\n",
        "                if i % save_point == 0 or i == epoch:\n",
        "                    saver.save(sess, os.path.join(checkpoint_dir_path, \"model\"), i)\n",
        "\n",
        "\n",
        "# test the model\n",
        "test_images = np.load('data/test_image.npy')\n",
        "test_labels = np.load('data/test_label.npy')\n",
        "checkpoint_path = 'data/model-30'\n",
        "\n",
        "true= 0\n",
        "X = tf.placeholder(tf.float32, [None, 50, 50, 1], name=\"normalized_gray_image\")\n",
        "output_0 = simple_model(X)\n",
        "init = tf.global_variables_initializer()\n",
        "saver = tf.train.Saver()\n",
        "with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
        "    sess.run(init)\n",
        "    saver.restore(sess, checkpoint_path)\n",
        "    for image, label in zip(test_images, test_labels):\n",
        "        prediction = sess.run(output_0, feed_dict={X: [image]})\n",
        "        print(\"prediction : \", prediction)\n",
        "        print(\"ground_truth : \", label)\n",
        "        pred = round(prediction[0][0])\n",
        "        pred = 0 if pred == 0 else 1\n",
        "        if pred == round(label[0]): true += 1\n",
        "        else:\n",
        "            cv2.imshow(\"test\", image)\n",
        "            cv2.waitKey(30)\n",
        "        image = image\n",
        "        message = \"c\" if round(prediction[0][0]) == 0 else \"o\"\n",
        "        cv2.putText(image, message, (0, 40), cv2.FONT_HERSHEY_COMPLEX_SMALL, 1, (0, 0, 255), 2)\n",
        "        message = \"c\" if label[0] == 0 else \"o\"\n",
        "        cv2.putText(image, message, (0, 10), cv2.FONT_HERSHEY_COMPLEX_SMALL, 1, (0, 255, 0), 2)\n",
        "\n",
        "print(\"true: \", true)\n",
        "\n",
        "\n",
        "# freeze model\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.framework.graph_util import convert_variables_to_constants\n",
        "\n",
        "checkpoint_path = \"data/model-30\"\n",
        "with tf.Graph().as_default():\n",
        "    X = tf.placeholder(tf.float32, [None, 50, 50, 1], name=\"normalized_gray_image\")\n",
        "    output_0 = simple_model(X, training=False)\n",
        "    saver = tf.train.Saver()\n",
        "    init = tf.global_variables_initializer()\n",
        "    with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
        "        sess.run(init)\n",
        "        saver.restore(sess, checkpoint_path)\n",
        "        minimal_graph = convert_variables_to_constants(sess, sess.graph_def, [\"final_output\"])\n",
        "        tf.io.write_graph(minimal_graph, '.', 'object_detection/model.pb', as_text=False)\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}